# üöÄ Projet : Construire et Tester une Infrastructure de Donn√©es
‚ú® Auteur : Ma√´va Beauvillain
üìÖ Date de d√©but : F√©vrier 2025
üìÖ Derni√®re MAJ : 11 mars 2025

## üìå Contexte
GreenAndCoop, un fournisseur coop√©ratif fran√ßais d'√©lectricit√© renouvelable, souhaite am√©liorer ses pr√©visions de demande d'√©lectricit√© en int√©grant des donn√©es m√©t√©orologiques issues de nouvelles sources (stations m√©t√©o amateurs et open-data). En tant que Data Engineer, ma mission est de construire un pipeline fiable permettant de collecter, transformer et stocker ces donn√©es dans MongoDB sur AWS.

## üèó Objectifs
- Collecter des donn√©es provenant de stations m√©t√©orologiques en open-data (InfoClimat, Weather Underground).
- Int√©grer ces donn√©es dans une base MongoDB, adapt√©e aux sp√©cificit√©s des diff√©rentes sources.
- Mettre en place un pipeline de collecte, traitement, et stockage des donn√©es.
- Assurer la qualit√© des donn√©es en analysant leur int√©grit√© (taux d'erreurs, donn√©es manquantes, etc.).
- D√©ployer cette solution sur AWS, en vue d'une int√©gration avec SageMaker pour les travaux de Machine Learning.

## üõ† Stack Technique
- **Langage** : Python 3.13
- **Gestion d‚Äôenvironnement** : Poetry
- **ETL** : Airbyte
- **Stockage** : AWS S3
- **Base de donn√©es** : MongoDB

## Schema de la BDD

```
{
    "id_station": "string",
    "station_info": {
        "name": "string",
        "latitude": "double",
        "longitude": "double",
        "elevation": "int",
        "city": "string",
        "state": "string",
        "type": "string",
        "hardware": "string",
        "software": "string",
        "license": {
            "license": "string",
            "url": "string",
            "source": "string",
            "metadonnees": "string"
        }
    },
    "datetime": "string",
    "weather_data": {
        "temperature": "double",
        "pressure": "double",
        "humidity": "int",
        "dew_point": "double",
        "visibility": "double",
        "wind_speed": "double",
        "wind_gust": "double",
        "wind_direction": "double",
        "precip_1h": "double",
        "precip_3h": "double",
        "precip_accum": "double",
        "precip_rate": "double",
        "solar": "double",
        "uv": "int",
        "snow_depth": "double",
        "nebulosity": "double",
        "weather_wmo": "double"
    }
}
```

## Dictionnaire des donn√©es
| Champs           | Indic                          | Unit√© de mesure |
|------------------|--------------------------------|------------------|
| id_station       | id of the station              |                  |
| datetime         | timestamp in format yyyy-mm-dd hh:mm:ss |              |
| temperature      | temperature                    | ¬∞C               |
| pressure         | mean sea level pressure        | hPa              |
| humidity         | relative humidity              | %                |
| dew_point        | dewpoint                       | ¬∞C               |
| visibility       | horizontal visibility          | m                |
| wind_speed       | mean wind speed                | km/h             |
| wind_gust        | wind gust                      | km/h             |
| wind_direction   | wind direction                 | degrees          |
| precip_1h        | precipitation over 1h          | mm               |
| precip_3h        | precipitation over 3h          | mm               |
| precip_accum     | accumulated precipitation      | mm               |
| precip_rate      | precipitation rate             | mm               |
| solar            | solar radiation                | W/m¬≤             |
| uv               | UV index                       | UV               |
| snow_depth       | snow depth                     | cm               |
| nebulosity       | cloud cover                    | octats           |
| weather_wmo      | present weather                |                  |


## Logique de transformation des donn√©es 
Ce script transforme des donn√©es m√©t√©orologiques extraites depuis des fichiers JSON situ√©s dans un bucket S3, avec les √©tapes suivantes :

### 1. Extraction des donn√©es depuis S3
- Connexion au bucket S3 p8-airbyte-greenandcoop.
- T√©l√©chargement du fichier JSON contenant des donn√©es m√©t√©orologiques infoclimat.
- Parsing du fichier JSON pour en extraire les informations relatives aux stations m√©t√©o sous la cl√© _airbyte_data.

### 2. Transformation des donn√©es des stations
- Pour chaque station, certains champs sont renomm√©s et de nouveaux champs sont ajout√©s (ex. city, state).
- Les champs des stations sont ensuite r√©organis√©s pour uniformiser les donn√©es.

### 3. Ajout des stations weather_underground
- Deux stations suppl√©mentaires provenant de Weather Underground sont ajout√©es √† la liste des stations existantes.

### 4. Sauvegarde sur S3
- Les donn√©es transform√©es sont converties en JSON et sauvegard√©es dans un nouveau fichier dans le bucket S3.

### 5. Transformation des donn√©es horaires (Weather Underground)
- Les jeux de donn√©es weather_underground (Belgique et France) sont extraits depuis S3, trait√©s pour ajouter et ajuster les dates et heures.
- Les colonnes Temperature, Dew Point, Pressure, Gust et autres sont converties aux unit√©s m√©triques (Celsius, km/h, hPa, mm).
- Les directions du vent sont converties en degr√©s.
- Les valeurs textuelles sont nettoy√©es de toute unit√© superflue (comme "¬∞F", "%").
- Le jeux de donn√©es infoclimat est extrait depuis S3
- Les noms et ordre des champs sont harmonis√©s

### 6. Conversion et sauvegarde sur S3
- Les donn√©es transform√©es sont converties en JSON et sauvegard√©es dans un nouveau fichier dans le bucket S3.

## üîß Infrastructure et D√©ploiement
- **Docker** : L‚Äôensemble des services, y compris MongoDB et les scripts de transformation, a √©t√© conteneuris√© √† l‚Äôaide de Docker Compose. Le fichier docker-compose.yml d√©finit un replica set MongoDB ainsi qu‚Äôun service data_pipeline qui ex√©cute les scripts Python.
- **R√©seau** : Tous les conteneurs sont connect√©s via un r√©seau Docker bridge. Les scripts de transformation interagissent avec MongoDB via un alias r√©seau interne.

## üîç Tests et Qualit√© des Donn√©es
- **V√©rification de l‚Äôint√©grit√©** : Les donn√©es sont v√©rifi√©es √† chaque √©tape du pipeline pour s‚Äôassurer que les champs requis sont pr√©sents, que les types de donn√©es sont corrects et que les valeurs manquantes sont g√©r√©es de mani√®re appropri√©e.
- **Tests automatis√©s** : Des tests unitaires et d‚Äôint√©gration ont √©t√© mis en place pour valider le sch√©ma de la base de donn√©es et les transformations de donn√©es. Ces tests s‚Äôassurent que les indices sont correctement appliqu√©s et que les performances sont optimales.
- **Analyse des performances** : Des tests d‚Äôaccessibilit√© ont √©t√© effectu√©s pour mesurer les temps de r√©ponse avec et sans index, afin de garantir un acc√®s rapide aux donn√©es pour les Data Scientists.
